{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0d1db8",
   "metadata": {},
   "source": [
    "## Paso 1: Instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a8b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (4.9.9)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (3.10.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pillow in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (11.2.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (4.2.2)\n",
      "Requirement already satisfied: promise in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (22.0.0)\n",
      "Requirement already satisfied: simple_parsing in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Requirement already satisfied: toml in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: einops in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.12.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
      "Requirement already satisfied: rich in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from dm-tree->tensorflow-datasets) (25.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from simple_parsing->tensorflow-datasets) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.72.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andy\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->tensorflow-datasets) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow-datasets matplotlib opencv-python pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e638",
   "metadata": {},
   "source": [
    "## Paso 2: Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92185ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf585de2",
   "metadata": {},
   "source": [
    "## Paso 3: Descargar dataset de perros y gatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ba5f1",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è SOLUCI√ìN ALTERNATIVA si hay problemas con el dataset\n",
    "\n",
    "Si el paso anterior falla, puedes usar esta soluci√≥n alternativa m√°s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ac2905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No se encontraron carpetas locales (esto es normal si es la primera vez)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVA: Descargar dataset manualmente desde Kaggle\n",
    "# Si el m√©todo anterior no funciona, ejecuta esto:\n",
    "\n",
    "# 1. Ve a: https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
    "# 2. Descarga el archivo ZIP\n",
    "# 3. Extr√°elo en una carpeta llamada 'PetImages' en la misma ubicaci√≥n de este notebook\n",
    "# 4. La estructura debe ser: PetImages/Cat/ y PetImages/Dog/\n",
    "\n",
    "# Funci√≥n para cargar desde carpetas locales\n",
    "def cargar_desde_carpetas_locales():\n",
    "    \"\"\"Carga im√°genes si tienes las carpetas PetImages/Cat y PetImages/Dog\"\"\"\n",
    "    datos_local = []\n",
    "    \n",
    "    carpetas = [\n",
    "        ('PetImages/Cat', 0),  # Gatos = 0\n",
    "        ('PetImages/Dog', 1)   # Perros = 1\n",
    "    ]\n",
    "    \n",
    "    for carpeta, etiqueta in carpetas:\n",
    "        if not os.path.exists(carpeta):\n",
    "            continue\n",
    "            \n",
    "        archivos = [f for f in os.listdir(carpeta) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Cargando {len(archivos)} im√°genes de {carpeta}...\")\n",
    "        \n",
    "        for i, archivo in enumerate(archivos[:5000]):  # M√°ximo 5000 por clase\n",
    "            try:\n",
    "                ruta = os.path.join(carpeta, archivo)\n",
    "                img = cv2.imread(ruta)\n",
    "                if img is not None:\n",
    "                    datos_local.append((img, etiqueta))\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Procesadas {i + 1}...\")\n",
    "    \n",
    "    return datos_local\n",
    "\n",
    "# Intentar cargar desde carpetas locales si existen\n",
    "if os.path.exists('PetImages'):\n",
    "    print(\"üìÅ Encontradas carpetas locales de PetImages\")\n",
    "    datos_local = cargar_desde_carpetas_locales()\n",
    "    if len(datos_local) > 0:\n",
    "        # Convertir a formato similar a tensorflow_datasets\n",
    "        print(f\"‚úì Cargadas {len(datos_local)} im√°genes localmente\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No se encontraron carpetas locales (esto es normal si es la primera vez)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927815cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\ANDY\\tensorflow_datasets\\cats_vs_dogs\\4.0.1 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ANDY\\tensorflow_datasets\\cats_vs_dogs\\4.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 824887076/824887076 [00:00<00:00, 155812977366.14 MiB/s]\n",
      "Dl Completed...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 144.19 url/s]\n",
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28msetattr\u001b[39m(tfds.image_classification.cats_vs_dogs, \u001b[33m'\u001b[39m\u001b[33m_URL\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Descargar el set de datos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m datos, metadatos = \u001b[43mtfds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcats_vs_dogs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal de im√°genes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadatos.splits[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].num_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\load.py:666\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs, file_format)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[32m    542\u001b[39m \n\u001b[32m    543\u001b[39m \u001b[33;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    658\u001b[39m \u001b[33;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[32m    660\u001b[39m dbuilder = _fetch_builder(\n\u001b[32m    661\u001b[39m     name=name,\n\u001b[32m    662\u001b[39m     data_dir=data_dir,\n\u001b[32m    663\u001b[39m     builder_kwargs=builder_kwargs,\n\u001b[32m    664\u001b[39m     try_gcs=try_gcs,\n\u001b[32m    665\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    669\u001b[39m   as_dataset_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\load.py:518\u001b[39m, in \u001b[36m_download_and_prepare_builder\u001b[39m\u001b[34m(dbuilder, download, download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m    517\u001b[39m   download_and_prepare_kwargs = download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m   \u001b[43mdbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1782\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1775\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils.tqdm(\n\u001b[32m   1776\u001b[39m       split_generators.items(),\n\u001b[32m   1777\u001b[39m       desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating splits...\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1778\u001b[39m       unit=\u001b[33m\"\u001b[39m\u001b[33m splits\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1779\u001b[39m       leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1780\u001b[39m   ):\n\u001b[32m   1781\u001b[39m     filename_template = \u001b[38;5;28mself\u001b[39m._get_filename_template(split_name=split_name)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m     future = \u001b[43msplit_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnondeterministic_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnondeterministic_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m     split_info_futures.append(future)\n\u001b[32m   1791\u001b[39m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\split_builder.py:447\u001b[39m, in \u001b[36mSplitBuilder.submit_split_generation\u001b[39m\u001b[34m(self, split_name, generator, filename_template, disable_shuffling, nondeterministic_order)\u001b[39m\n\u001b[32m    442\u001b[39m     logging.warning(\n\u001b[32m    443\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m`nondeterministic_order` is set to True for a dataset that does\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m not use beam. Setting `disable_shuffling` to True.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    445\u001b[39m     )\n\u001b[32m    446\u001b[39m     build_kwargs[\u001b[33m'\u001b[39m\u001b[33mdisable_shuffling\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[32m    449\u001b[39m   unknown_generator_type = \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    450\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    451\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mExpected generator or apache_beam object. Got: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    452\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    453\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\core\\split_builder.py:508\u001b[39m, in \u001b[36mSplitBuilder._build_from_generator\u001b[39m\u001b[34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[39m\n\u001b[32m    498\u001b[39m serialized_info = \u001b[38;5;28mself\u001b[39m._features.get_serialized_info()\n\u001b[32m    499\u001b[39m writer = writer_lib.Writer(\n\u001b[32m    500\u001b[39m     serializer=example_serializer.ExampleSerializer(serialized_info),\n\u001b[32m    501\u001b[39m     filename_template=filename_template,\n\u001b[32m   (...)\u001b[39m\u001b[32m    506\u001b[39m     ignore_duplicates=\u001b[38;5;28mself\u001b[39m._ignore_duplicates,\n\u001b[32m    507\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGenerating \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m examples...\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m examples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_num_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmininterval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_datasets\\image_classification\\cats_vs_dogs.py:119\u001b[39m, in \u001b[36mCatsVsDogs._generate_examples\u001b[39m\u001b[34m(self, archive)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m zipfile.ZipFile(buffer, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m new_zip:\n\u001b[32m    118\u001b[39m   new_zip.writestr(norm_fname, img_recoded.numpy())\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m new_fobj = \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m record = {\n\u001b[32m    122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: new_fobj,\n\u001b[32m    123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage/filename\u001b[39m\u001b[33m\"\u001b[39m: norm_fname,\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: label,\n\u001b[32m    125\u001b[39m }\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m norm_fname, record\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\zipfile\\__init__.py:1639\u001b[39m, in \u001b[36mZipFile.open\u001b[39m\u001b[34m(self, name, mode, pwd, force_zip64)\u001b[39m\n\u001b[32m   1636\u001b[39m     zinfo.compress_level = \u001b[38;5;28mself\u001b[39m.compresslevel\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1638\u001b[39m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1639\u001b[39m     zinfo = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\zipfile\\__init__.py:1567\u001b[39m, in \u001b[36mZipFile.getinfo\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1565\u001b[39m info = \u001b[38;5;28mself\u001b[39m.NameToInfo.get(name)\n\u001b[32m   1566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   1568\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m in the archive\u001b[39m\u001b[33m'\u001b[39m % name)\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[31mKeyError\u001b[39m: \"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\""
     ]
    }
   ],
   "source": [
    "# Correcci√≥n temporal para el dataset\n",
    "setattr(tfds.image_classification.cats_vs_dogs, '_URL',\n",
    "        \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n",
    "\n",
    "# Descargar el set de datos\n",
    "datos, metadatos = tfds.load('cats_vs_dogs', as_supervised=True, with_info=True)\n",
    "print(f\"Total de im√°genes: {metadatos.splits['train'].num_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66f28d",
   "metadata": {},
   "source": [
    "## Paso 4: IMPORTANTE - Crear carpeta con tus fotos\n",
    "\n",
    "**INSTRUCCIONES:**\n",
    "1. Crea una carpeta llamada `mis_fotos` en la misma ubicaci√≥n que este notebook\n",
    "2. Agrega al menos 50-100 fotos tuyas (diferentes √°ngulos, iluminaci√≥n, fondos)\n",
    "3. Las fotos pueden ser JPG, PNG, etc.\n",
    "4. Pueden ser selfies, fotos de tu cara en diferentes situaciones\n",
    "\n",
    "Ruta esperada: `clasificador-perros-gatos-main/mis_fotos/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tama√±o de imagen\n",
    "TAMANO_IMG = 100\n",
    "\n",
    "# Verificar que existe la carpeta con tus fotos\n",
    "CARPETA_MIS_FOTOS = 'mis_fotos'\n",
    "\n",
    "if not os.path.exists(CARPETA_MIS_FOTOS):\n",
    "    print(f\"‚ö†Ô∏è ERROR: No existe la carpeta '{CARPETA_MIS_FOTOS}'\")\n",
    "    print(f\"Por favor, crea la carpeta y agrega tus fotos all√≠.\")\n",
    "else:\n",
    "    num_fotos = len([f for f in os.listdir(CARPETA_MIS_FOTOS) \n",
    "                     if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "    print(f\"‚úì Carpeta encontrada con {num_fotos} fotos\")\n",
    "    if num_fotos < 50:\n",
    "        print(f\"‚ö†Ô∏è ADVERTENCIA: Se recomienda al menos 50 fotos. Tienes {num_fotos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee3751",
   "metadata": {},
   "source": [
    "## Paso 5: Cargar y procesar tus fotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_mis_fotos(carpeta, tamano_img=100):\n",
    "    \"\"\"Carga y procesa las fotos de la carpeta especificada\"\"\"\n",
    "    imagenes = []\n",
    "    extensiones_validas = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    \n",
    "    archivos = [f for f in os.listdir(carpeta) if f.lower().endswith(extensiones_validas)]\n",
    "    \n",
    "    print(f\"Procesando {len(archivos)} im√°genes...\")\n",
    "    \n",
    "    for i, archivo in enumerate(archivos):\n",
    "        try:\n",
    "            ruta = os.path.join(carpeta, archivo)\n",
    "            # Leer imagen\n",
    "            imagen = cv2.imread(ruta)\n",
    "            if imagen is None:\n",
    "                print(f\"‚ö†Ô∏è No se pudo leer: {archivo}\")\n",
    "                continue\n",
    "            \n",
    "            # Redimensionar\n",
    "            imagen = cv2.resize(imagen, (tamano_img, tamano_img))\n",
    "            # Convertir a escala de grises\n",
    "            imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "            # Reshape para el formato del modelo\n",
    "            imagen = imagen.reshape(tamano_img, tamano_img, 1)\n",
    "            \n",
    "            imagenes.append(imagen)\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Procesadas {i + 1}/{len(archivos)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error al procesar {archivo}: {e}\")\n",
    "    \n",
    "    print(f\"‚úì Total procesadas: {len(imagenes)}\")\n",
    "    return np.array(imagenes)\n",
    "\n",
    "# Cargar tus fotos\n",
    "mis_imagenes = cargar_mis_fotos(CARPETA_MIS_FOTOS, TAMANO_IMG)\n",
    "print(f\"\\nShape de tus im√°genes: {mis_imagenes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20deb63",
   "metadata": {},
   "source": [
    "## Paso 6: Visualizar algunas de tus fotos procesadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar algunas de tus fotos procesadas\n",
    "plt.figure(figsize=(15, 5))\n",
    "num_mostrar = min(10, len(mis_imagenes))\n",
    "for i in range(num_mostrar):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(mis_imagenes[i].reshape(TAMANO_IMG, TAMANO_IMG), cmap='gray')\n",
    "    plt.title(\"T√∫\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91e7cf",
   "metadata": {},
   "source": [
    "## Paso 7: Procesar dataset de perros y gatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todas las im√°genes del dataset\n",
    "datos_entrenamiento = []\n",
    "\n",
    "print(\"Procesando im√°genes de perros y gatos...\")\n",
    "for i, (imagen, etiqueta) in enumerate(datos['train']):\n",
    "    # Redimensionar y convertir a escala de grises\n",
    "    imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))\n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "    imagen = imagen.reshape(TAMANO_IMG, TAMANO_IMG, 1)\n",
    "    \n",
    "    # Guardar imagen con su etiqueta original (0=gato, 1=perro)\n",
    "    datos_entrenamiento.append([imagen, etiqueta.numpy()])\n",
    "    \n",
    "    if (i + 1) % 2000 == 0:\n",
    "        print(f\"  Procesadas {i + 1} im√°genes...\")\n",
    "\n",
    "print(f\"‚úì Total de perros y gatos: {len(datos_entrenamiento)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf53c35",
   "metadata": {},
   "source": [
    "## Paso 8: Combinar todos los datos (gatos, perros y t√∫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar tus fotos al dataset con etiqueta 2\n",
    "for imagen in mis_imagenes:\n",
    "    datos_entrenamiento.append([imagen, 2])  # Etiqueta 2 = T√∫\n",
    "\n",
    "print(f\"Total de im√°genes en el dataset combinado: {len(datos_entrenamiento)}\")\n",
    "\n",
    "# Mezclar los datos aleatoriamente\n",
    "np.random.shuffle(datos_entrenamiento)\n",
    "print(\"‚úì Datos mezclados aleatoriamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61380c38",
   "metadata": {},
   "source": [
    "## Paso 9: Separar im√°genes y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045852ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar variables X (entradas) y y (etiquetas)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for imagen, etiqueta in datos_entrenamiento:\n",
    "    X.append(imagen)\n",
    "    y.append(etiqueta)\n",
    "\n",
    "# Convertir a arrays numpy\n",
    "X = np.array(X).astype(float) / 255  # Normalizar\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "print(f\"  Gatos (0): {np.sum(y == 0)}\")\n",
    "print(f\"  Perros (1): {np.sum(y == 1)}\")\n",
    "print(f\"  T√∫ (2): {np.sum(y == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9c6a4",
   "metadata": {},
   "source": [
    "## Paso 10: Visualizar ejemplos de las 3 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar ejemplos de cada clase\n",
    "plt.figure(figsize=(15, 5))\n",
    "clases = ['Gato', 'Perro', 'T√∫']\n",
    "\n",
    "for clase_idx in range(3):\n",
    "    # Encontrar √≠ndices de esta clase\n",
    "    indices = np.where(y == clase_idx)[0][:5]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(3, 5, clase_idx * 5 + i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(X[idx].reshape(TAMANO_IMG, TAMANO_IMG), cmap='gray')\n",
    "        if i == 0:\n",
    "            plt.ylabel(clases[clase_idx], fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b54922",
   "metadata": {},
   "source": [
    "## Paso 11: Crear el modelo CNN para 3 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd82ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN para 3 clases\n",
    "modelo = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(250, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 clases con softmax\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Para m√∫ltiples clases\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeacfb3",
   "metadata": {},
   "source": [
    "## Paso 12: Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "history = modelo.fit(\n",
    "    X, y,\n",
    "    batch_size=32,\n",
    "    validation_split=0.15,\n",
    "    epochs=50,  # Puedes ajustar seg√∫n necesites\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2786",
   "metadata": {},
   "source": [
    "## Paso 13: Visualizar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar accuracy y loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validaci√≥n')\n",
    "plt.title('Precisi√≥n del Modelo')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Validaci√≥n')\n",
    "plt.title('P√©rdida del Modelo')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bff7cf",
   "metadata": {},
   "source": [
    "## Paso 14: Probar el modelo con ejemplos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7be0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar con im√°genes aleatorias\n",
    "clases_nombres = ['Gato', 'Perro', 'T√∫']\n",
    "indices_prueba = np.random.choice(len(X), 12, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, idx in enumerate(indices_prueba):\n",
    "    # Hacer predicci√≥n\n",
    "    imagen_test = X[idx].reshape(1, 100, 100, 1)\n",
    "    prediccion = modelo.predict(imagen_test, verbose=0)\n",
    "    clase_predicha = np.argmax(prediccion)\n",
    "    confianza = prediccion[0][clase_predicha] * 100\n",
    "    \n",
    "    # Mostrar imagen\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(X[idx].reshape(TAMANO_IMG, TAMANO_IMG), cmap='gray')\n",
    "    \n",
    "    # Color verde si es correcto, rojo si no\n",
    "    color = 'green' if clase_predicha == y[idx] else 'red'\n",
    "    plt.title(f\"Real: {clases_nombres[y[idx]]}\\nPred: {clases_nombres[clase_predicha]} ({confianza:.1f}%)\",\n",
    "              color=color, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb78732",
   "metadata": {},
   "source": [
    "## Paso 15: Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "modelo.save('clasificador_personalizado.h5')\n",
    "print(\"‚úì Modelo guardado como 'clasificador_personalizado.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b97901",
   "metadata": {},
   "source": [
    "## Paso 16: Convertir a TensorFlow.js (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar tensorflowjs\n",
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta y convertir\n",
    "!mkdir modelo_web\n",
    "!tensorflowjs_converter --input_format keras clasificador_personalizado.h5 modelo_web\n",
    "print(\"‚úì Modelo convertido para web en la carpeta 'modelo_web'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c7cdf",
   "metadata": {},
   "source": [
    "## Paso 17: Funci√≥n para probar con una imagen nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_imagen(ruta_imagen):\n",
    "    \"\"\"Predice la clase de una imagen desde un archivo\"\"\"\n",
    "    # Leer y procesar imagen\n",
    "    imagen = cv2.imread(ruta_imagen)\n",
    "    imagen = cv2.resize(imagen, (TAMANO_IMG, TAMANO_IMG))\n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "    imagen = imagen.reshape(1, TAMANO_IMG, TAMANO_IMG, 1)\n",
    "    imagen = imagen.astype(float) / 255\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    prediccion = modelo.predict(imagen, verbose=0)\n",
    "    clase = np.argmax(prediccion)\n",
    "    confianza = prediccion[0][clase] * 100\n",
    "    \n",
    "    clases_nombres = ['Gato', 'Perro', 'T√∫']\n",
    "    \n",
    "    print(f\"\\nPredicci√≥n: {clases_nombres[clase]}\")\n",
    "    print(f\"Confianza: {confianza:.2f}%\")\n",
    "    print(f\"\\nProbabilidades:\")\n",
    "    for i, prob in enumerate(prediccion[0]):\n",
    "        print(f\"  {clases_nombres[i]}: {prob*100:.2f}%\")\n",
    "    \n",
    "    # Mostrar imagen\n",
    "    img_mostrar = cv2.imread(ruta_imagen)\n",
    "    img_mostrar = cv2.cvtColor(img_mostrar, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_mostrar)\n",
    "    plt.title(f\"Predicci√≥n: {clases_nombres[clase]} ({confianza:.1f}%)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso (descomenta y cambia la ruta):\n",
    "# predecir_imagen('mis_fotos/foto1.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
